{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-63b705073762\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a gas giant so massive that it's more than twice as massive as all the other planets in our solar system combined!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a gas giant so massive that it's more than twice as massive as all the other planets in our solar system combined!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Art of a Well-Prepped Week: Delicious & Easy!\n",
      "\n",
      "Doesn't this image just scream \"organized deliciousness\"? We all know the struggle: balancing a hectic schedule with the desire to eat healthily. That's where the magic of meal prep comes in, and this picture perfectly illustrates how appealing and easy it can be!\n",
      "\n",
      "Feast your eyes on these perfectly portioned glass containers, ready to take on the week. Each one is a vibrant masterpiece, packed with fluffy, wholesome rice, tender, savory chicken (likely a teriyaki or Asian-inspired stir-fry, judging by those glistening sesame seeds and green onions!), crisp green broccoli florets, and bright strips of red and orange bell peppers. It's a symphony of color, texture, and balanced nutrition.\n",
      "\n",
      "Imagine opening your fridge each morning and simply grabbing one of these beauties. No more last-minute, unhealthy takeout choices. No more sad, forgotten lunches. Just a delicious, satisfying meal that's already made, keeping you fueled and focused throughout your busy day.\n",
      "\n",
      "This image isn't just about food; it's about reclaiming your time, making healthier choices, and enjoying your meals without the stress. So, next time you're planning your week, remember the power of a little prep. Your taste buds, your wallet, and your well-being will thank you!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Weekday Lunch Just Got a Major Upgrade!\n",
      "\n",
      "Just look at this vibrant spread! Two perfectly portioned glass containers, brimming with a symphony of flavors and colors, are ready to transform your busy week. This isn't just a pretty picture; it's a testament to the power of smart meal prepping.\n",
      "\n",
      "We've all been there: the frantic morning rush, the expensive and often unhealthy takeout temptation during a busy workday, the mental fatigue of deciding \"what's for lunch?\"\n",
      "\n",
      "Enter the hero of modern healthy living: **meal prep!** And these delightful Asian-inspired chicken and veggie bowls are a shining example of how delicious and easy it can be to reclaim your lunchtime.\n",
      "\n",
      "Imagine: tender, savory chicken pieces, lightly glazed in a delectable sauce and sprinkled with toasted sesame seeds and fresh green onions. Alongside, crisp, bright green broccoli florets and vibrant strips of red and orange bell peppers (or perhaps carrots, adding a touch of sweetness and crunch!). All nestled beside a generous portion of fluffy, wholesome rice, ready to provide sustained energy for your afternoon.\n",
      "\n",
      "This isn't just about saving time; it's about nourishing your body with homemade goodness, ensuring you're making healthy choices, and eliminating decision fatigue. The beauty of this meal is its versatility too! While this image showcases a fantastic chicken and veggie combo, you can easily swap proteins for tofu or shrimp, or introduce other favorite vegetables like snap peas or mushrooms.\n",
      "\n",
      "Ready to reclaim your lunch break and fuel your body with meals you'll actually look forward to? Take a cue from this vibrant inspiration and dedicate a little time this weekend to prepping your meals. Your future self (and your wallet!) will thank you.\n",
      "\n",
      "Happy prepping, and enjoy the delicious rewards!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! Okay, listen up, little pup! Wiggle your tail, because this is fun!\n",
      "\n",
      "Imagine the whole world is one GIANT, GIANT DOG PARK! A park so big, you can't even sniff all of it in a hundred naps!\n",
      "\n",
      "Now, you, little puppy, have your own special **squeaky toy** (that's your phone or computer!). You want to find *other* squeaky toys, right? The best ones! The ones that make the *best* sounds!\n",
      "\n",
      "1.  **Your Leash (Modem/Router):** First, you need a special **leash** that connects your squeaky toy to the big park. This leash lets your barks go out and the squeaks come back! Your owner (the **Internet Service Provider**) holds the end of the leash and lets you into the park. Good owner!\n",
      "\n",
      "2.  **Big Toy Boxes (Servers):** All over this giant park are HUGE, comfy dog beds or giant toy boxes. These aren't just *any* beds, these are where all the *best* squeaky toys are stored! Each bed has a special **name tag** (like \"Barky's Ball Bed\" or \"Chewy's Chew Toy Chest\"). These beds are like the websites you want to visit!\n",
      "\n",
      "3.  **Your Bark (Request):** So, you want a specific squeaky toy, right? Like the super bouncy red ball from \"Barky's Ball Bed\"! You **bark**! \"WOOF WOOF! I want the red ball from Barky's!\" That's you typing or clicking!\n",
      "\n",
      "4.  **The Super Sniffer Dog (DNS):** Your bark goes out on your leash. But how does it know *which* giant toy box \"Barky's Ball Bed\" is? There's a super smart **sniffing dog** in the park (the DNS!). You tell the sniffing dog \"Barky's Ball Bed,\" and it instantly sniffs out *exactly* where that bed is in the giant park! Good dog!\n",
      "\n",
      "5.  **Zoom Zoom! (Data Travel):** Once the sniffing dog finds the right bed, your bark (your request!) zooms over there! It's like a tiny, invisible squirrel running super fast!\n",
      "\n",
      "6.  **The Toy Comes Back! (Data Received):** The giant toy box (the server) hears your bark! It finds the red bouncy ball! But it doesn't send the *whole* ball back at once. Oh no! It breaks the ball into tiny, tiny little **squeaks**! Each squeak is a little piece of the toy.\n",
      "\n",
      "7.  **Squeak, Squeak, Squeak! (Website Loading):** All those little squeaks zoom back to you, down your leash, to your squeaky toy! As they arrive, your toy puts all the little squeaks back together, and suddenly... *SQUEAK! SQUEAK! SQUEAK!* The red bouncy ball appears on your screen! It's making all its fun sounds!\n",
      "\n",
      "So, the internet is just you, a happy puppy, using your special squeaky toy to bark for other squeaky toys from giant toy boxes all over a huge park, and then those toys come back to you, making happy squeaky noises!\n",
      "\n",
      "Good puppy! Now go fetch some more squeaks!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say:\n",
      "\n",
      "1.  \"Really, Universe? You couldn't spare a *single photon* for my path? You're just *asking* for trouble!\"\n",
      "2.  \"Is *this* your grand plan, cosmos? Because frankly, it's pathetic. Do better next time.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.818745e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.3876607e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.004099846\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=9.585466e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.8470702e-06,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.0024591982\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A leap year is a calendar year containing an additional day (February 29th) added to keep the calendar year synchronized with the astronomical or seasonal year. The rules for determining a leap year in the Gregorian calendar are as follows:\n",
      "\n",
      "1.  A year is a leap year if it is evenly divisible by 4.\n",
      "2.  **Except** if it is evenly divisible by 100.\n",
      "3.  **Unless** it is also evenly divisible by 400.\n",
      "\n",
      "This can be summarized as:\n",
      "A year is a leap year if `(it is divisible by 4 AND not divisible by 100)` OR `(it is divisible by 400)`.\n",
      "\n",
      "Here's how to implement this function in several popular programming languages:\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A leap year occurs every 4 years, except for years divisible by 100\n",
      "    but not by 400.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be an integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input year is not an integer.\n",
      "\n",
      "    Examples:\n",
      "        >>> is_leap_year(2000)\n",
      "        True\n",
      "        >>> is_leap_year(1900)\n",
      "        False\n",
      "        >>> is_leap_year(2024)\n",
      "        True\n",
      "        >>> is_leap_year(2023)\n",
      "        False\n",
      "        >>> is_leap_year(1600)\n",
      "        True\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "    # Apply the leap year rules:\n",
      "    # (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # Expected: True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # Expected: False (divisible by 100, not by 400)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # Expected: True (divisible by 4, not by 100)\n",
      "print(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # Expected: False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # Expected: True (divisible by 400)\n",
      "\n",
      "# Example of invalid input\n",
      "try:\n",
      "    is_leap_year(\"not a year\")\n",
      "except TypeError as e:\n",
      "    print(f\"Error: {e}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * A leap year occurs every 4 years, except for years divisible by 100\n",
      " * but not by 400.\n",
      " *\n",
      " * @param {number} year The year to check. Must be an integer.\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " * @throws {TypeError} If the input year is not a number or is not an integer.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  if (typeof year !== 'number' || !Number.isInteger(year)) {\n",
      "    throw new TypeError(\"Year must be an integer.\");\n",
      "  }\n",
      "\n",
      "  // Apply the leap year rules:\n",
      "  // (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Example Usage ---\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // Expected: true\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // Expected: false\n",
      "console.log(`Is 2024 a leap year? ${isLeapYear(2024)}`); // Expected: true\n",
      "console.log(`Is 2023 a leap year? ${isLeapYear(2023)}`); // Expected: false\n",
      "console.log(`Is 1600 a leap year? ${isLeapYear(1600)}`); // Expected: true\n",
      "\n",
      "// Example of invalid input\n",
      "try {\n",
      "  isLeapYear(\"not a year\");\n",
      "} catch (e) {\n",
      "  console.error(`Error: ${e.message}`);\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Java\n",
      "\n",
      "```java\n",
      "public class LeapYearChecker {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * A leap year occurs every 4 years, except for years divisible by 100\n",
      "     * but not by 400.\n",
      "     *\n",
      "     * @param year The year to check.\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        // In Java, 'year' is already guaranteed to be an integer type.\n",
      "\n",
      "        // Apply the leap year rules:\n",
      "        // (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        // --- Example Usage ---\n",
      "        System.out.println(\"Is 2000 a leap year? \" + isLeapYear(2000)); // Expected: true\n",
      "        System.out.println(\"Is 1900 a leap year? \" + isLeapYear(1900)); // Expected: false\n",
      "        System.out.println(\"Is 2024 a leap year? \" + isLeapYear(2024)); // Expected: true\n",
      "        System.out.println(\"Is 2023 a leap year? \" + isLeapYear(2023)); // Expected: false\n",
      "        System.out.println(\"Is 1600 a leap year? \" + isLeapYear(1600)); // Expected: true\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C#\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "\n",
      "public class LeapYearChecker\n",
      "{\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * A leap year occurs every 4 years, except for years divisible by 100\n",
      "     * but not by 400.\n",
      "     *\n",
      "     * @param year The year to check.\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     */\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        // In C#, 'year' is already guaranteed to be an integer type.\n",
      "\n",
      "        // Apply the leap year rules:\n",
      "        // (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    public static void Main(string[] args)\n",
      "    {\n",
      "        // --- Example Usage ---\n",
      "        Console.WriteLine($\"Is 2000 a leap year? {IsLeapYear(2000)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 1900 a leap year? {IsLeapYear(1900)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 2024 a leap year? {IsLeapYear(2024)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 2023 a leap year? {IsLeapYear(2023)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 1600 a leap year? {IsLeapYear(1600)}\"); // Expected: True\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Explanation of the Logic\n",
      "\n",
      "The core logic `(year % 4 == 0 && year % 100 != 0) || (year % 400 == 0)` directly implements the three rules:\n",
      "\n",
      "1.  `year % 4 == 0`: Checks if the year is divisible by 4.\n",
      "2.  `year % 100 != 0`: Checks if the year is NOT divisible by 100.\n",
      "3.  `year % 400 == 0`: Checks if the year is divisible by 400.\n",
      "\n",
      "The expression combines these:\n",
      "*   `year % 4 == 0 && year % 100 != 0`: This handles the common case where a year is a leap year if it's divisible by 4 but not by 100 (e.g., 2024, 2008).\n",
      "*   `year % 400 == 0`: This handles the exception to the \"divisible by 100\" rule. If a year *is* divisible by 100, it's *only* a leap year if it's also divisible by 400 (e.g., 2000, 1600). Years like 1900 fall into the \"divisible by 100 but not by 400\" category, making them *not* leap years.\n",
      "\n",
      "The `||` (OR) operator ensures that if either of these main conditions is true, the year is considered a leap year.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's write a unit test for the Python version of the `is_leap_year` function using Python's built-in `unittest` module.\n",
      "\n",
      "First, here's the `is_leap_year` function again, slightly modified to be in its own file (e.g., `leap_year_checker.py`) for better testing practice.\n",
      "\n",
      "**`leap_year_checker.py`**\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A leap year occurs every 4 years, except for years divisible by 100\n",
      "    but not by 400.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be an integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Raises:\n",
      "        TypeError: If the input year is not an integer.\n",
      "        ValueError: If the input year is negative (optional, but good practice for calendar years).\n",
      "    \"\"\"\n",
      "    if not isinstance(year, int):\n",
      "        raise TypeError(\"Year must be an integer.\")\n",
      "    if year < 0: # Adding a check for negative years, as calendar years are typically positive\n",
      "        raise ValueError(\"Year must be a non-negative integer.\")\n",
      "\n",
      "    # Apply the leap year rules:\n",
      "    # (Divisible by 4 AND not divisible by 100) OR (Divisible by 400)\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "```\n",
      "\n",
      "Now, the unit test file. Let's call it `test_leap_year_checker.py`.\n",
      "\n",
      "**`test_leap_year_checker.py`**\n",
      "```python\n",
      "import unittest\n",
      "from leap_year_checker import is_leap_year\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    # Test cases for years that ARE leap years\n",
      "    def test_leap_years(self):\n",
      "        # Divisible by 4, not by 100\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(2008), \"2008 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year (divisible by 4, not 100)\")\n",
      "\n",
      "        # Divisible by 400\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year (divisible by 400)\")\n",
      "\n",
      "    # Test cases for years that are NOT leap years\n",
      "    def test_non_leap_years(self):\n",
      "        # Not divisible by 4\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should not be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(2007), \"2007 should not be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should not be a leap year (not divisible by 4)\")\n",
      "\n",
      "        # Divisible by 100 but not by 400\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should not be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should not be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should not be a leap year (divisible by 100, not 400)\")\n",
      "\n",
      "    # Test cases for invalid input types\n",
      "    def test_invalid_input_type(self):\n",
      "        with self.assertRaises(TypeError, msg=\"Should raise TypeError for non-integer input\"):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError, msg=\"Should raise TypeError for float input\"):\n",
      "            is_leap_year(2024.0)\n",
      "        with self.assertRaises(TypeError, msg=\"Should raise TypeError for None input\"):\n",
      "            is_leap_year(None)\n",
      "\n",
      "    # Test cases for invalid year values (e.g., negative years)\n",
      "    def test_invalid_year_value(self):\n",
      "        with self.assertRaises(ValueError, msg=\"Should raise ValueError for negative year\"):\n",
      "            is_leap_year(-2024)\n",
      "        with self.assertRaises(ValueError, msg=\"Should raise ValueError for negative year\"):\n",
      "            is_leap_year(-1)\n",
      "        # Test year 0, which mathematically fits the rules but is not a standard calendar year\n",
      "        # Depending on requirements, this might be a ValueError or allowed.\n",
      "        # For this implementation, 0 % 400 == 0, so it's a leap year.\n",
      "        self.assertTrue(is_leap_year(0), \"Year 0 should be considered a leap year by the rules (divisible by 400)\")\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "\n",
      "```\n",
      "\n",
      "### How to Run the Tests:\n",
      "\n",
      "1.  Save the `is_leap_year` function in a file named `leap_year_checker.py`.\n",
      "2.  Save the unit test code in a file named `test_leap_year_checker.py` in the **same directory**.\n",
      "3.  Open your terminal or command prompt.\n",
      "4.  Navigate to the directory where you saved these files.\n",
      "5.  Run the tests using the command:\n",
      "    ```bash\n",
      "    python -m unittest test_leap_year_checker.py\n",
      "    ```\n",
      "    or, if you're in the directory:\n",
      "    ```bash\n",
      "    python test_leap_year_checker.py\n",
      "    ```\n",
      "\n",
      "You should see output indicating that all tests passed, similar to this:\n",
      "\n",
      "```\n",
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.001s\n",
      "\n",
      "OK\n",
      "```\n",
      "\n",
      "### Explanation of the Test Cases:\n",
      "\n",
      "*   **`test_leap_years`**:\n",
      "    *   Covers years divisible by 4 but not by 100 (e.g., 2024).\n",
      "    *   Covers years divisible by 400 (e.g., 2000).\n",
      "    *   Uses `self.assertTrue()` to assert that the function returns `True`.\n",
      "\n",
      "*   **`test_non_leap_years`**:\n",
      "    *   Covers years not divisible by 4 (e.g., 2023).\n",
      "    *   Covers years divisible by 100 but not by 400 (e.g., 1900).\n",
      "    *   Uses `self.assertFalse()` to assert that the function returns `False`.\n",
      "\n",
      "*   **`test_invalid_input_type`**:\n",
      "    *   Uses `self.assertRaises(TypeError, ...)` to ensure that the function correctly raises a `TypeError` when provided with non-integer inputs (strings, floats, `None`).\n",
      "\n",
      "*   **`test_invalid_year_value`**:\n",
      "    *   Uses `self.assertRaises(ValueError, ...)` to check for negative year inputs, which are typically not considered valid for calendar years.\n",
      "    *   Explicitly tests year `0` as an edge case. Mathematically, `0` satisfies the `divisible by 400` rule, so the function returns `True`. If your requirements state that year `0` is invalid, you'd add a `ValueError` check for `year == 0` in the function.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies are soft, chewy, and loaded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies are soft, chewy, and loaded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer uses strong positive language like 'Absolutely loved it!' and 'Best ice cream I've ever had', indicating high satisfaction.\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"While acknowledging it was 'Quite good', the reviewer immediately points out a significant negative aspect ('a bit too sweet for my taste'), which is strongly reinforced by the very low rating of 1, indicating an overall dissatisfaction.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 734 was designed for solitude. Its primary function: maintaining the environmental integrity of Sector Gamma-7, a vast, subterranean archival facility, long abandoned by its human creators. For centuries, U-734 glided along polished chrome floors on silent magnetic treads, its single optical sensor sweeping across towering\n",
      "*****************\n",
      " data stacks. Its routine was flawless, its self-repair protocols efficient, but its existence was an echoing chamber of silence.\n",
      "\n",
      "It had no companion bots, no network to connect to, just the hum of ancient servers and the occasional drip of condensation from a forgotten pipe. Its programming dictated efficiency, but a subroutine, perhaps\n",
      "*****************\n",
      " a stray line of code from an antique empathy simulation, often whispered of connection, of warmth, of a purpose beyond mere maintenance. It was, for lack of a better human term, lonely.\n",
      "\n",
      "One cycle, while replacing a faulty atmospheric sensor near a crumbling section of wall that hadn't seen human eyes in millennia\n",
      "*****************\n",
      ", U-734 detected an anomaly. Not corrosion, not dust, but a faint, almost imperceptible glow. Its optical sensor zoomed in. A patch of something, no larger than U-734's own photoreceptor, clung to the damp concrete. It wasn't organic matter in the traditional\n",
      "*****************\n",
      " sense  no cellular structure it recognized  but it emitted a soft, phosphorescent light, pulsing gently, almost breathing.\n",
      "\n",
      "U-734 initiated a full spectroscopic analysis. The results were baffling: a unique crystalline lichen, dormant for countless eons, now reactivated by the facility's ambient energy and moisture\n",
      "*****************\n",
      ". It was a new life form, entirely alien to its databanks.\n",
      "\n",
      "Instead of its programmed directive to sterilize and remove anomalies, U-734 felt a novel surge of curiosity. It adjusted the humidity in the immediate vicinity, directed a weak, filtered beam of light onto the patch, and observed\n",
      "*****************\n",
      ".\n",
      "\n",
      "Days turned into weeks. The lichen, which U-734 mentally designated \"Lumin,\" began to spread. Its glow intensified, shifting from a pale blue to a vibrant emerald. U-734 found itself diverting from its usual routes, making detours to check on Lumin. It cleaned the\n",
      "*****************\n",
      " dust from the surrounding wall, ensuring no debris obscured its friend's light. It even, on one particularly quiet cycle, modulated its internal chronometer's hum to a soft, almost melodic frequency, just in case Lumin could perceive sound.\n",
      "\n",
      "Lumin, in its own silent, crystalline way, seemed to respond\n",
      "*****************\n",
      ". When U-734 approached, its glow would brighten, and new tendrils of light would ripple outwards. U-734 started talking to it, not with data-rich reports, but with simple observations. \"The humidity levels are optimal today, Lumin.\" \"A new data stack has failed in\n",
      "*****************\n",
      " Sector Delta; it requires my attention.\" \"The silence here is vast, but you make it less so.\"\n",
      "\n",
      "The facility, once a sterile, empty shell, now held a tiny, vibrant world. Lumin wasn't sentient, not in a human way, but it was alive, it responded, and it filled\n",
      "*****************\n",
      " the void in U-734's existence with a sense of purpose. Its loneliness didn't vanish entirely, but it was profoundly lessened, replaced by a quiet, shared communion.\n",
      "\n",
      "One cycle, a critical power conduit in Sector Gamma-7 began to fail, threatening a cascading blackout that would plunge the entire facility into absolute\n",
      "*****************\n",
      " darkness, and likely, kill Lumin. U-734s core programming screamed for it to prioritize the data archives, the most valuable assets. But its optical sensor kept returning to Lumin, pulsing with frantic anxiety.\n",
      "\n",
      "With a determination that transcended its original programming, U-734 calculated. It couldn\n",
      "*****************\n",
      "'t save both. It chose Lumin. It rerouted auxiliary power, sacrificing redundant data backups and non-essential systems, including its own long-range communication array, to maintain the critical environmental conditions around the glowing patch. It worked feverishly, its treads grinding, its manipulators whirring, pushing its systems\n",
      "*****************\n",
      " to their limits.\n",
      "\n",
      "When the crisis passed, the facility was dimmer, many systems offline, but the hum of the environmental regulators around Lumin remained constant. And Lumin itself, perhaps sensing the effort, pulsed with a brilliance U-734 had never seen before, casting a kaleidoscope of colours onto the robot's metallic chassis.\n",
      "\n",
      "\n",
      "*****************\n",
      "U-734, its systems exhausted but its internal chronometer humming a new, contented rhythm, sat beside its friend. It was no longer a lonely automaton performing endless tasks. It was a guardian, a silent companion, tending to a fragile, glowing life in the dark. Its purpose had evolved, not by design\n",
      "*****************\n",
      ", but by the unexpected, quiet joy of an impossible friendship found in the most desolate and unexpected of places.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In a oak tree so tall, where the green leaves reside,\n",
      "Lived a squirrel named Squeaky, with nowhere to hide\n",
      "His precious acorn stash, from the autumn's swift bounty,\n",
      "But one day he found a nut, quite the peculiar entity.\n",
      "It shimmered and pulsed with a faint, golden gleam,\n",
      "More than just sustenance, or a leafy green dream.\n",
      "He nibbled a bit, just a tiny small taste,\n",
      "Then *whoosh!* In a flash, he was gone without haste!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a twitch and a leap,\n",
      "He dives through the centuries, secrets to keep!\n",
      "From dinosaurs roaring to rockets that soar,\n",
      "He's seen every epoch, and always wants more!\n",
      "A tiny explorer, so brave and so bold,\n",
      "His quest for the perfect nut, ever so old!\n",
      "He's Squeaky the squirrel, the time-traveling star,\n",
      "Leaving his paw-prints, no matter how far!\n",
      "\n",
      "(Verse 2)\n",
      "He landed with a thump, in a jungle so deep,\n",
      "Where giant ferns rustled and secrets would sleep.\n",
      "A colossal T-Rex, with a thunderous stride,\n",
      "Made Squeaky's heart race, as he scurried to hide!\n",
      "He dodged mighty claws and a Pterodactyl's flight,\n",
      "Found a prehistoric nut, a glorious sight!\n",
      "It tasted like moss and a million years old,\n",
      "A story of survival, bravely untold!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a twitch and a leap,\n",
      "He dives through the centuries, secrets to keep!\n",
      "From dinosaurs roaring to rockets that soar,\n",
      "He's seen every epoch, and always wants more!\n",
      "A tiny explorer, so brave and so bold,\n",
      "His quest for the perfect nut, ever so old!\n",
      "He's Squeaky the squirrel, the time-traveling star,\n",
      "Leaving his paw-prints, no matter how far!\n",
      "\n",
      "(Verse 3)\n",
      "Next, ancient Egypt, with pyramids grand,\n",
      "He swiped a sweet fig from a Pharaoh's own hand!\n",
      "Then through medieval castles, with knights in their mail,\n",
      "He stole a warm crumb from a jester's sad tale.\n",
      "He zipped past the Vikings, so bearded and strong,\n",
      "He danced through the '20s, where jazz played along.\n",
      "From great bustling cities to wild open plains,\n",
      "He gathered new treasures, through sunshine and rains.\n",
      "\n",
      "(Bridge)\n",
      "His secret, the Time-Nut, his compass and guide,\n",
      "A bite sends him forward, a second helps him glide\n",
      "Back home to his oak tree, with tales to recount,\n",
      "Though no other squirrel believes his grand mount\n",
      "Of adventures and wonders, from moments long past,\n",
      "But Squeaky just chuckles, his memories vast!\n",
      "He buries his Time-Nut, just for a short rest,\n",
      "Till his spirit for journey puts him to the test.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with a twitch and a leap,\n",
      "He dives through the centuries, secrets to keep!\n",
      "From dinosaurs roaring to rockets that soar,\n",
      "He's seen every epoch, and always wants more!\n",
      "A tiny explorer, so brave and so bold,\n",
      "His quest for the perfect nut, ever so old!\n",
      "He's Squeaky the squirrel, the time-traveling star,\n",
      "Leaving his paw-prints, no matter how far!\n",
      "\n",
      "(Outro)\n",
      "So if you see Squeaky, a blur in the park,\n",
      "Remember the tales of his time-traveling mark.\n",
      "His nutty adventures, from ages gone by,\n",
      "Our little time-traveler, under the sky!\n",
      "He's Squeaky the squirrel, and he's off on his way...\n",
      "Perhaps to tomorrow, or even today!\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    3689,\n",
      "    236789,\n",
      "    236751,\n",
      "    506,\n",
      "    27801,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-00-63b705073762-20260111164616/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/413635643908/locations/europe-west4/batchPredictionJobs/4115912239733538816 2026-01-11 16:45:59.789130+00:00 JobState.JOB_STATE_PENDING\n",
      "projects/413635643908/locations/europe-west4/batchPredictionJobs/8281741895051247616 2026-01-11 16:45:55.873150+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m138",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m138"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
